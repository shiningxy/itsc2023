{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>准备数据</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>读取数据</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:06:46.575642Z",
     "start_time": "2021-10-20T12:06:44.024789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 10:11:43.546008: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-13 10:11:43.546035: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-13 10:11:43.546054: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-13 10:11:43.550595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 10:11:44.103640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from time import localtime, strftime\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, Flatten, BatchNormalization\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.regularizers import L1L2, L1, L2\n",
    "from tensorflow.keras.callbacks import History, TensorBoard\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>设置tensorflow</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:06:46.710633Z",
     "start_time": "2021-10-20T12:06:46.590645Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 10:11:46.671809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 10:11:46.696226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 10:11:46.696328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "for gpu in tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>处理数据</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>读取数据</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:06:47.934271Z",
     "start_time": "2021-10-20T12:06:47.871641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.053</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.8</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.64</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>9.9</td>\n",
       "      <td>0.095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>5.3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4750 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      特征0    特征1   特征2   特征3   特征4   特征5   特征6   特征7   特征8   特征9  ...  特征16  \\\n",
       "0     7.4  0.048  1.19  1.47  0.23  0.54  0.27  5.75  0.65  0.71  ...  0.07   \n",
       "1     3.7    NaN  1.87  0.18  0.94  0.70  0.07  5.20  0.73  0.59  ...  0.91   \n",
       "2     1.2  0.046  1.97  1.17  0.67  0.89  0.60  2.10  0.97  0.14  ...  0.80   \n",
       "3     7.2  0.053  1.17  2.13  0.82  0.18  0.17  5.30  0.27  0.46  ...  0.37   \n",
       "4     1.8  0.021  1.89  0.42  0.20  0.92  0.80  1.60  0.20  0.70  ...  0.50   \n",
       "...   ...    ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "4745  5.4  0.069  1.77  0.99  0.39  0.54  0.64  4.95  0.70  0.30  ...  0.23   \n",
       "4746  8.0  0.078  1.87  2.91  0.93  0.13  0.51  2.50  0.56  0.09  ...  0.09   \n",
       "4747  6.9  0.051  1.45  0.48  0.17  0.33  0.13  3.10  0.56  0.99  ...  0.06   \n",
       "4748  9.9  0.095   NaN  2.79  0.08  0.06  0.75  5.20  0.02  0.69  ...  0.44   \n",
       "4749  5.3  0.005  1.31  2.22  0.64  0.34  0.26  1.70  0.16  0.57  ...  0.02   \n",
       "\n",
       "      特征17   补偿0   补偿1   补偿2   补偿3   补偿4   补偿5   补偿6   补偿7  \n",
       "0     0.62  1.23  0.88  0.73  0.19  1.67  0.36  1.58  1.62  \n",
       "1     0.89  0.99  1.11  0.13  0.18  1.33  0.86  1.22  1.30  \n",
       "2     0.69  1.85  0.13  1.54  1.91  0.63  0.70  1.71  1.57  \n",
       "3     0.37  1.72  0.58  1.03  1.77  0.86  1.65  1.95  1.37  \n",
       "4     0.68  0.83  0.09  0.94  0.49  1.87  1.21  1.53  1.56  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "4745  0.89  0.28  0.46  0.03  0.73  0.13  0.82  0.02  0.22  \n",
       "4746  0.23  0.25  0.76  0.82  0.05  0.79  0.44  0.21  0.92  \n",
       "4747  0.69  0.60  0.59  0.54  0.95  0.97  0.67  0.73  0.33  \n",
       "4748  0.84  0.48  0.85  0.26  0.24  0.35  0.10   NaN  0.07  \n",
       "4749  0.16  0.98  0.02  0.38  0.80  0.56  0.93  0.07  0.21  \n",
       "\n",
       "[4750 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/pic/adjustment.tsv\", delimiter=\"\\t\", skipinitialspace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>处理缺失值</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:06:53.370192Z",
     "start_time": "2021-10-20T12:06:53.357158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>特征0</th>\n",
       "      <th>特征1</th>\n",
       "      <th>特征2</th>\n",
       "      <th>特征3</th>\n",
       "      <th>特征4</th>\n",
       "      <th>特征5</th>\n",
       "      <th>特征6</th>\n",
       "      <th>特征7</th>\n",
       "      <th>特征8</th>\n",
       "      <th>特征9</th>\n",
       "      <th>...</th>\n",
       "      <th>特征16</th>\n",
       "      <th>特征17</th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.053</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.8</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>9.9</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.59</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.64</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>5.3</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4361 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      特征0    特征1   特征2   特征3   特征4   特征5   特征6   特征7   特征8   特征9  ...  特征16  \\\n",
       "0     7.4  0.048  1.19  1.47  0.23  0.54  0.27  5.75  0.65  0.71  ...  0.07   \n",
       "2     1.2  0.046  1.97  1.17  0.67  0.89  0.60  2.10  0.97  0.14  ...  0.80   \n",
       "3     7.2  0.053  1.17  2.13  0.82  0.18  0.17  5.30  0.27  0.46  ...  0.37   \n",
       "4     1.8  0.021  1.89  0.42  0.20  0.92  0.80  1.60  0.20  0.70  ...  0.50   \n",
       "5     7.6  0.097  1.89  0.75  0.91  0.01  0.82  4.60  0.20  0.03  ...  0.16   \n",
       "...   ...    ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "4744  9.9  0.059  1.79  2.73  0.83  0.12  0.59  4.45  0.72  0.39  ...  0.21   \n",
       "4745  5.4  0.069  1.77  0.99  0.39  0.54  0.64  4.95  0.70  0.30  ...  0.23   \n",
       "4746  8.0  0.078  1.87  2.91  0.93  0.13  0.51  2.50  0.56  0.09  ...  0.09   \n",
       "4747  6.9  0.051  1.45  0.48  0.17  0.33  0.13  3.10  0.56  0.99  ...  0.06   \n",
       "4749  5.3  0.005  1.31  2.22  0.64  0.34  0.26  1.70  0.16  0.57  ...  0.02   \n",
       "\n",
       "      特征17   补偿0   补偿1   补偿2   补偿3   补偿4   补偿5   补偿6   补偿7  \n",
       "0     0.62  1.23  0.88  0.73  0.19  1.67  0.36  1.58  1.62  \n",
       "2     0.69  1.85  0.13  1.54  1.91  0.63  0.70  1.71  1.57  \n",
       "3     0.37  1.72  0.58  1.03  1.77  0.86  1.65  1.95  1.37  \n",
       "4     0.68  0.83  0.09  0.94  0.49  1.87  1.21  1.53  1.56  \n",
       "5     0.25  0.62  0.33  1.13  1.67  1.12  1.23  1.55  1.22  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "4744  0.43  0.83  0.45  0.51  0.05  0.53  0.20  0.76  0.24  \n",
       "4745  0.89  0.28  0.46  0.03  0.73  0.13  0.82  0.02  0.22  \n",
       "4746  0.23  0.25  0.76  0.82  0.05  0.79  0.44  0.21  0.92  \n",
       "4747  0.69  0.60  0.59  0.54  0.95  0.97  0.67  0.73  0.33  \n",
       "4749  0.16  0.98  0.02  0.38  0.80  0.56  0.93  0.07  0.21  \n",
       "\n",
       "[4361 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:06:54.844553Z",
     "start_time": "2021-10-20T12:06:54.831025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>补偿0</th>\n",
       "      <th>补偿1</th>\n",
       "      <th>补偿2</th>\n",
       "      <th>补偿3</th>\n",
       "      <th>补偿4</th>\n",
       "      <th>补偿5</th>\n",
       "      <th>补偿6</th>\n",
       "      <th>补偿7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.23</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.85</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    补偿0   补偿1   补偿2   补偿3   补偿4   补偿5   补偿6   补偿7\n",
       "0  1.23  0.88  0.73  0.19  1.67  0.36  1.58  1.62\n",
       "2  1.85  0.13  1.54  1.91  0.63  0.70  1.71  1.57\n",
       "3  1.72  0.58  1.03  1.77  0.86  1.65  1.95  1.37\n",
       "4  0.83  0.09  0.94  0.49  1.87  1.21  1.53  1.56\n",
       "5  0.62  0.33  1.13  1.67  1.12  1.23  1.55  1.22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data.iloc[:, 0:18]\n",
    "y = data.iloc[:, 18:]\n",
    "x.head(5)\n",
    "y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>划分验证集</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:06:58.245861Z",
     "start_time": "2021-10-20T12:06:58.238898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3488, 18)\n",
      "(873, 18)\n",
      "(3488, 8)\n",
      "(873, 8)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>搭建模型并训练</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>搭建模型</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T12:55:49.410383Z",
     "start_time": "2021-10-18T12:55:49.332372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"adjustment\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bn_0 (BatchNormalization)   (None, 18)                72        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               2432      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36560 (142.81 KB)\n",
      "Trainable params: 36524 (142.67 KB)\n",
      "Non-trainable params: 36 (144.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(name=\"bn_0\", input_shape=(x.shape[1], )),\n",
    "    Dense(units=128, activation=LeakyReLU(), kernel_regularizer=L2(0.2), activity_regularizer=L1(0.1), name=\"dense_1\"),\n",
    "    Dense(units=128, activation=LeakyReLU(), kernel_regularizer=L2(0.2), activity_regularizer=L1(0.1), name=\"dense_2\"),\n",
    "    Dense(units=128, activation=LeakyReLU(), kernel_regularizer=L2(0.2), activity_regularizer=L1(0.1), name=\"dense_3\"),\n",
    "    Dropout(0.2, name=\"dropout_1\"),\n",
    "    Dense(units=y.shape[1], activation=None, name=\"dense_8\")\n",
    "], name=\"adjustment\")\n",
    "\n",
    "model.compile(\n",
    "    loss=Huber(),\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>训练模型</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T12:56:55.444116Z",
     "start_time": "2021-10-18T12:55:52.010013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1470 - mae: 0.4512 - val_loss: 0.1432 - val_mae: 0.4487\n",
      "Epoch 2/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1446 - mae: 0.4512 - val_loss: 0.1419 - val_mae: 0.4487\n",
      "Epoch 3/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1442 - mae: 0.4512 - val_loss: 0.1416 - val_mae: 0.4487\n",
      "Epoch 4/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1439 - mae: 0.4512 - val_loss: 0.1415 - val_mae: 0.4487\n",
      "Epoch 5/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1438 - mae: 0.4512 - val_loss: 0.1413 - val_mae: 0.4488\n",
      "Epoch 6/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1437 - mae: 0.4512 - val_loss: 0.1412 - val_mae: 0.4488\n",
      "Epoch 7/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1436 - mae: 0.4512 - val_loss: 0.1412 - val_mae: 0.4488\n",
      "Epoch 8/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1436 - mae: 0.4512 - val_loss: 0.1411 - val_mae: 0.4488\n",
      "Epoch 9/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1435 - mae: 0.4512 - val_loss: 0.1411 - val_mae: 0.4487\n",
      "Epoch 10/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1435 - mae: 0.4512 - val_loss: 0.1411 - val_mae: 0.4488\n",
      "Epoch 11/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1434 - mae: 0.4512 - val_loss: 0.1410 - val_mae: 0.4487\n",
      "Epoch 12/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1434 - mae: 0.4512 - val_loss: 0.1410 - val_mae: 0.4488\n",
      "Epoch 13/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1434 - mae: 0.4512 - val_loss: 0.1410 - val_mae: 0.4487\n",
      "Epoch 14/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1433 - mae: 0.4512 - val_loss: 0.1410 - val_mae: 0.4488\n",
      "Epoch 15/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1433 - mae: 0.4512 - val_loss: 0.1409 - val_mae: 0.4488\n",
      "Epoch 16/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1433 - mae: 0.4512 - val_loss: 0.1409 - val_mae: 0.4488\n",
      "Epoch 17/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1433 - mae: 0.4512 - val_loss: 0.1409 - val_mae: 0.4488\n",
      "Epoch 18/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1433 - mae: 0.4512 - val_loss: 0.1409 - val_mae: 0.4487\n",
      "Epoch 19/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1433 - mae: 0.4512 - val_loss: 0.1409 - val_mae: 0.4488\n",
      "Epoch 20/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1433 - mae: 0.4512 - val_loss: 0.1409 - val_mae: 0.4488\n",
      "Epoch 21/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 22/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 23/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 24/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 25/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 26/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 27/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 28/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 29/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 30/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1432 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 31/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 32/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1408 - val_mae: 0.4488\n",
      "Epoch 33/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 34/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 35/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 36/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 37/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 38/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 39/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 40/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 41/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 42/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1431 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 43/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 44/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 45/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1407 - val_mae: 0.4488\n",
      "Epoch 46/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 47/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 48/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 49/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 50/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 51/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 52/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 53/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 54/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 55/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1430 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 56/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1406 - val_mae: 0.4488\n",
      "Epoch 57/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 58/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 59/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 60/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 61/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 62/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 63/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 64/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 65/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1429 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 66/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 67/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1405 - val_mae: 0.4488\n",
      "Epoch 68/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 69/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 70/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 71/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 72/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 73/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 74/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4487\n",
      "Epoch 75/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 76/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 77/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 78/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1428 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 79/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4488\n",
      "Epoch 80/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 81/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 82/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1404 - val_mae: 0.4487\n",
      "Epoch 83/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 84/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 85/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 86/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 87/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 88/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 89/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 90/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 91/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 92/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 93/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 94/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 95/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 96/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 97/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 98/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 99/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4487\n",
      "Epoch 100/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 101/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 102/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 103/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4487\n",
      "Epoch 104/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 105/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4487\n",
      "Epoch 106/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 107/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 108/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 109/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 110/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 111/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 112/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 113/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 114/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 115/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 116/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 117/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 118/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 119/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 120/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 121/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 122/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 123/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 124/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 125/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 126/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 127/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 128/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 129/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 130/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 131/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 132/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 133/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 134/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 135/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 136/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 137/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 138/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 139/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 140/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 141/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 142/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 143/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 144/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 145/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 146/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 147/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 148/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 149/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 150/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 151/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 152/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 153/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 154/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 155/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 156/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 157/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 158/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4487\n",
      "Epoch 159/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 160/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 161/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 162/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 163/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 164/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 165/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 166/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 167/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 168/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 169/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 170/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 171/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 172/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 173/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 174/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 175/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 176/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 177/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 178/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 179/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 180/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 181/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 182/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 183/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 184/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 185/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 186/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 187/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 188/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 189/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 190/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 191/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 192/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1426 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 193/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 194/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 195/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 196/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1426 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 197/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1426 - mae: 0.4512 - val_loss: 0.1402 - val_mae: 0.4488\n",
      "Epoch 198/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 199/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 200/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 201/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 202/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4487\n",
      "Epoch 203/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 204/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 205/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1426 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 206/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 207/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 208/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 209/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 210/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 211/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 212/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 213/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 214/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 215/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 216/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 217/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 218/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 219/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 220/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 221/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 222/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 223/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 224/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 225/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 226/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 227/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 228/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 229/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 230/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 231/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 232/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 233/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 234/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 235/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 236/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 237/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 238/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 239/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1402 - val_mae: 0.4487\n",
      "Epoch 240/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 241/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 242/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 243/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 244/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 245/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 246/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 247/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 248/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 249/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 250/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 251/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 252/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 253/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 254/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 255/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 256/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 257/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 258/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 259/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 260/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 261/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 262/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 263/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 264/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4487\n",
      "Epoch 265/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 266/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 267/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 268/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 269/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 270/300\n",
      "109/109 [==============================] - 1s 9ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 271/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 272/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 273/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 274/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 275/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 276/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 277/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 278/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 279/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 280/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 281/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 282/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 283/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 284/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 285/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 286/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 287/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4487\n",
      "Epoch 288/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 289/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 290/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 291/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 292/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 293/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 294/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4487\n",
      "Epoch 295/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 296/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 297/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 298/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 299/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n",
      "Epoch 300/300\n",
      "109/109 [==============================] - 1s 8ms/step - loss: 0.1427 - mae: 0.4512 - val_loss: 0.1403 - val_mae: 0.4488\n"
     ]
    }
   ],
   "source": [
    "history: History = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=300,\n",
    "    verbose=1,\n",
    "    workers=-1,\n",
    "    use_multiprocessing=True,\n",
    "    callbacks=[\n",
    "        TensorBoard(log_dir=\"../logs/\" + strftime(\"%Y%m%d-%H%M%S\", localtime()))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 15:50:10.374790: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-11 15:50:10.374822: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-11 15:50:10.374839: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-11 15:50:10.378980: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-11 15:50:10.872645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-11-11 15:50:11.407669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 15:50:11.428131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-11 15:50:11.428246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.9.0 at http://csnt-ubuntu:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ../logs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T12:58:50.713750Z",
     "start_time": "2021-10-18T12:58:49.929749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/wxy/Industrial-Algorithm-Competition/models/adjustment/2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/wxy/Industrial-Algorithm-Competition/models/adjustment/2/assets\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"/home/wxy/Industrial-Algorithm-Competition/models/adjustment/2\")     # save_model(model, \"../models/adjustment/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>模型部署</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>部署服务</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for wxy: \n"
     ]
    }
   ],
   "source": [
    "!sudo docker run -t \\\n",
    "    -p 9989:9989 \\\n",
    "    -v \"/home/wxy/Industrial-Algorithm-Competition/models/adjustment/2:/models/adjustment/2\" \\\n",
    "    -e MODEL_NAME=adjustment \\\n",
    "    tensorflow/serving:2.14.0-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>测试数据</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>读取数据并测试</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:07:08.724340Z",
     "start_time": "2021-10-20T12:07:08.719378Z"
    }
   },
   "outputs": [],
   "source": [
    "data = x_test.values.tolist()\n",
    "headers = {\"content-type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'172.17.0.3\\n'\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\"echo 123|sudo -S docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' 2615fe3d76fd\" ,shell=True, stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "print(result.stdout) # 输出为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:07:09.569356Z",
     "start_time": "2021-10-20T12:07:09.423095Z"
    }
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    url=\"http://172.17.0.3:8501/v1/models/adjustment:predict\",\n",
    "    data=json.dumps({\"instances\": data}),\n",
    "    headers=headers\n",
    ")\n",
    "if response.status_code != 200:\n",
    "    raise ValueError(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:07:09.952405Z",
     "start_time": "2021-10-20T12:07:09.943407Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.array(json.loads(response.text)[\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T12:07:10.811326Z",
     "start_time": "2021-10-20T12:07:10.806323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2806500963324274\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
